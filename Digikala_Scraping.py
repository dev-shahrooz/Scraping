# -*- coding: utf-8 -*-
"""
Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§ - Ø¢ÛŒÙÙˆÙ†â€ŒÙ‡Ø§: Ù¾ÛŒÙ…Ø§ÛŒØ´ ØµÙØ­Ù‡ Ø¨Ù‡ ØµÙØ­Ù‡ Ø¨Ø§ Ù„Ø§Ú¯ Ùˆ Ø¯ÛŒØ¨Ø§Ú¯ Ù…Ø±Ø­Ù„Ù‡â€ŒØ¨Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡
"""
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time, json, re, os
from urllib.parse import urljoin

# ======== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ========
BASE_URL = "https://www.digikala.com"
CATEGORY_URL = "https://www.digikala.com/search/category-mobile-phone/apple/?sort=1&page={page}"
OUT_FILE = "digikala_apple_phones_all_pages.json"
MAX_PAGES = 10            # Ú†Ù†Ø¯ ØµÙØ­Ù‡ Ø¨Ø±ÙˆØ¯ØŸ
HEADLESS = True           # Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ù…Ø±ÙˆØ±Ú¯Ø±: False
SAVE_DEBUG_EVERY_PAGE = False  # Ø§Ú¯Ø± True Ø¨Ø§Ø´Ø¯ Ø§Ø² Ù‡Ø± ØµÙØ­Ù‡ HTML/PNG Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# =========================

PERSIAN_MAP = str.maketrans("Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹Ù¬", "0123456789,")  # Ø§Ø±Ù‚Ø§Ù… + ÙˆÛŒØ±Ú¯ÙˆÙ„ ÙØ§Ø±Ø³ÛŒ

def to_int_price(text: str):
    if not text:
        return None
    t = text.strip().translate(PERSIAN_MAP)
    if any(bt in t for bt in ("%", "Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯", "Ù…Ø´Ø§Ù‡Ø¯Ù‡", "Ø±Ø§ÛŒÚ¯Ø§Ù†", "free")):
        return None
    has_unit = ("ØªÙˆÙ…Ø§Ù†" in t) or ("Ø±ÛŒØ§Ù„" in t)
    nums = re.findall(r"\d[\d,]{4,}", t) if has_unit else re.findall(r"\d[\d,]{6,}", t)
    if not nums:
        return None
    try:
        return int(nums[-1].replace(",", ""))
    except:
        return None

def setup_driver():
    opts = Options()
    if HEADLESS:
        opts.add_argument("--headless=new")
    opts.add_argument("--disable-blink-features=AutomationControlled")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1400,900")
    return webdriver.Chrome(options=opts)

def save_artifacts(driver, prefix):
    """HTML Ùˆ Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ú©Ù† Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯."""
    try:
        with open(f"{prefix}.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
    except Exception as e:
        print(f"  [debug] Ù†ÙˆØ´ØªÙ† HTML Ø®Ø·Ø§: {e}")
    try:
        driver.save_screenshot(f"{prefix}.png")
    except Exception as e:
        print(f"  [debug] Ú¯Ø±ÙØªÙ† Ø§Ø³Ú©Ø±ÛŒÙ†â€ŒØ´Ø§Øª Ø®Ø·Ø§: {e}")

def close_popups_best_effort(driver):
    """Ù¾Ø§Ù¾â€ŒØ¢Ù¾â€ŒÙ‡Ø§/Ø¨Ù†Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø¨Ù†Ø¯ (best-effort)"""
    xps = [
        "//button[contains(text(),'Ø¨Ø§Ø´Ù‡') or contains(text(),'Ù‚Ø¨ÙˆÙ„') or contains(text(),'Ø¨Ø³ØªÙ†') or contains(text(),'Ù„ØºÙˆ') or contains(text(),'Ø±Ø¯ Ú©Ø±Ø¯Ù†')]",
        "//*[@aria-label='Ø¨Ø³ØªÙ†' or @aria-label='close' or @aria-label='Close']",
        "//*[@data-testid='modal-close' or @data-testid='close-button']",
        "//div[@role='dialog']//button",
    ]
    hit = False
    for xp in xps:
        try:
            els = driver.find_elements(By.XPATH, xp)
            for el in els[:3]:
                el.click()
                time.sleep(0.2)
                hit = True
        except:  # noqa
            pass
    # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø§ÙˆÙˆØ±Ù„ÛŒ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† Ø¢Ø®Ø±ÛŒÙ† Ø±Ø§Ù‡
    js_remove = r"""
    (function(){
      try{
        const sels=['[role="dialog"]','.modal,.MuiModal-root,.MuiDialog-root,.overlay,.Backdrop,.backdrop','#modal-root,#modal,#popup,#newsletter,#cookie'];
        let n=0; for(const s of sels){ document.querySelectorAll(s).forEach(e=>{e.remove(); n++;}); }
        document.body.style.overflow='auto'; return n;
      }catch(e){return 0;}
    })();
    """
    try:
        removed = driver.execute_script(js_remove)
        if removed:
            hit = True
    except:  # noqa
        pass
    return hit

def log_counts(driver, label=""):
    anchors = driver.find_elements(By.CSS_SELECTOR, "a[href^='/product/']")
    cards_by_testid = driver.find_elements(By.XPATH, "//*[@data-testid='product-card']")
    cards_custom = driver.find_elements(
        By.XPATH,
        "//div[contains(@class,'product-list_') and @data-product-index]//a[starts-with(@href,'/product/')]/div[@data-testid='product-card']/ancestor::a[1]"
    )
    print(f"  [{label}] anchors: {len(anchors)} | product-card: {len(cards_by_testid)} | our-cards: {len(cards_custom)}")
    return len(anchors), len(cards_by_testid), len(cards_custom)

def extract_price_from_card(card_el):
    # 1) Ù‚ÛŒÙ…Øª Ø§ØµÙ„ÛŒ
    for el in card_el.find_elements(By.XPATH, ".//*[@data-testid='price-final']"):
        p = to_int_price(el.text)
        if p: return p
    # 2) Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ price
    for el in card_el.find_elements(By.XPATH, ".//*[contains(@class,'price')]"):
        p = to_int_price(el.text)
        if p: return p
    # 3) fallback
    return to_int_price(card_el.text or "")

def extract_products_from_dom(driver):
    """ÙÙ‚Ø· Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø®ÙˆØ¯Ù Ø¯ÛŒØ¬ÛŒâ€ŒÚ©Ø§Ù„Ø§ Ø¨Ù‡â€ŒØµÙˆØ±Øª product-card Ø¯Ø§Ø¯Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…."""
    items, seen = [], set()
    cards = driver.find_elements(By.XPATH, "//*[@data-testid='product-card']/ancestor::a[1]")
    for a in cards:
        href = a.get_attribute("href")
        if not href or href in seen:
            continue
        seen.add(href)
        # Ø¹Ù†ÙˆØ§Ù†
        name = ""
        for xp in [".//h3", ".//h2", ".//*[@data-testid='product-title']"]:
            els = a.find_elements(By.XPATH, xp)
            if els:
                name = re.sub(r"\s+", " ", (els[0].text or "").strip())
                if name:
                    break
        if not name:
            name = re.sub(r"\s+", " ", (a.text or "").strip())
        # Ù‚ÛŒÙ…Øª
        price = extract_price_from_card(a)
        items.append({
            "name": name if name else href.rsplit("/", 1)[-1],
            "url": urljoin(BASE_URL, href),
            "price": price,
            "currency": "IRR"
        })
    return items

def scrape_page(driver, page_num, save_debug=False):
    url = CATEGORY_URL.format(page=page_num)
    print(f"\nğŸŸ© ØµÙØ­Ù‡ {page_num} â†’ {url}")
    driver.get(url)

    # ØµØ¨Ø± ØªØ§ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§ Ø­Ø§Ø¶Ø± Ø´ÙˆÙ†Ø¯ (ÛŒØ§ Ø­Ø¯Ø§Ù‚Ù„ body Ø¨ÛŒØ§ÛŒØ¯)
    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
    except:
        print("  [warn] body Ø¯ÛŒØ± Ø¢Ù…Ø¯.")
    closed = close_popups_best_effort(driver)
    if closed:
        print("  [info] Ù¾Ø§Ù¾â€ŒØ¢Ù¾â€ŒÙ‡Ø§/Ø§ÙˆÙˆØ±Ù„ÛŒâ€ŒÙ‡Ø§ Ø¨Ø³ØªÙ‡/Ø­Ø°Ù Ø´Ø¯.")

    # Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬: Ø´Ù…Ø§Ø±Ø´ Ø¹Ù†Ø§ØµØ± Ú©Ù„ÛŒØ¯ÛŒ
    log_counts(driver, label="before-wait")

    # ØµØ¨Ø± Ù…Ø®ØµÙˆØµ Ú©Ø§Ø±Øªâ€ŒÙ‡Ø§
    try:
        WebDriverWait(driver, 12).until(
            EC.presence_of_element_located((By.XPATH, "//*[@data-testid='product-card']"))
        )
    except:
        print("  [warn] product-card Ø¯ÛŒØ¯Ù‡ Ù†Ø´Ø¯.")
    time.sleep(1.0)

    # Ø´Ù…Ø§Ø±Ø´ Ù…Ø¬Ø¯Ø¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
    a_cnt, pc_cnt, oc_cnt = log_counts(driver, label="after-wait")
    if save_debug or (pc_cnt == 0 and oc_cnt == 0):
        prefix = f"page{page_num:02d}"
        save_artifacts(driver, prefix)
        print(f"  [debug] {prefix}.html Ùˆ {prefix}.png Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬
    products = extract_products_from_dom(driver)
    print(f"  [ok] Ù…Ø­ØµÙˆÙ„Ø§ØªÙ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ Ø¯Ø± DOM Ø§ÛŒÙ† ØµÙØ­Ù‡: {len(products)}")
    # Ù†Ù…ÙˆÙ†Ù‡â€ŒÛŒ 3 Ù…ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ
    for i, p in enumerate(products[:3], 1):
        print(f"    #{i} {p['name'][:60]} ... | price={p['price']} | url={p['url']}")
    return products

def main():
    driver = setup_driver()
    all_by_url = {}
    try:
        for page in range(1, MAX_PAGES + 1):
            products = scrape_page(driver, page, save_debug=SAVE_DEBUG_EVERY_PAGE)
            # Ø§Ø¯ØºØ§Ù…
            new_added = 0
            for p in products:
                if p["url"] not in all_by_url:
                    all_by_url[p["url"]] = p
                    new_added += 1
            print(f"  [sum] ØµÙØ­Ù‡ {page}: {len(products)} Ù…ÙˆØ±Ø¯ØŒ Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: {new_added}, Ù…Ø¬Ù…ÙˆØ¹ Ú©Ù„: {len(all_by_url)}")

            # Ø´Ø±Ø· ØªÙˆÙ‚Ù: Ø§Ú¯Ø± Ù‡ÛŒÚ† Ù…Ø­ØµÙˆÙ„ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ø´Ø¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¯Ø§Ø¯Ù† Ø¨ÛŒâ€ŒÙØ§ÛŒØ¯Ù‡ Ø§Ø³Øª
            if new_added == 0:
                print("  ğŸš© Ù…Ø­ØµÙˆÙ„ Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù†Ø´Ø¯. Ù¾ÛŒÙ…Ø§ÛŒØ´ Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
                break
            time.sleep(0.8)
    finally:
        driver.quit()

    all_products = list(all_by_url.values())
    with open(OUT_FILE, "w", encoding="utf-8") as f:
        json.dump(all_products, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“¦ Ù…Ø¬Ù…ÙˆØ¹ Ù†Ù‡Ø§ÛŒÛŒ: {len(all_products)}")
    print(f"ğŸ“ ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ: {OUT_FILE}")

if __name__ == "__main__":
    main()
